{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sgd_regression import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    indice = int(data.shape[0] * 0.7)\n",
    "    \n",
    "    shuffle_data = np.random.permutation(data)\n",
    "\n",
    "    treino = shuffle_data[0:indice, :] \n",
    "    teste =  shuffle_data[indice:,:]\n",
    "    \n",
    "    X_treino = treino[:,:-1]\n",
    "    y_treino = treino[:, -1]\n",
    "    \n",
    "    X_teste = teste[:,:-1]\n",
    "    y_teste = teste[:, -1]\n",
    "    \n",
    "    return X_treino, y_treino, X_teste, y_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset_vehicle.csv'\n",
    "X_treino, y_treino, X_teste, y_teste = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treino.shape[0] + X_teste.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separa_classes(X_treino, y_treino):\n",
    "    vans = []\n",
    "    opels = []\n",
    "    saabs = []\n",
    "    buss = []\n",
    "    for i in range(len(y_treino)):\n",
    "        if(y_treino[i] == 'van'):\n",
    "            vans.append(X_treino[i])\n",
    "        elif y_treino[i] == 'opel':\n",
    "            opels.append(X_treino[i])\n",
    "        elif y_treino[i] == 'bus':\n",
    "            buss.append(X_treino[i])\n",
    "        else:\n",
    "            saabs.append(X_treino[i])\n",
    "            \n",
    "    return np.array(vans), np.array(opels), np.array(saabs), np.array(buss)\n",
    "\n",
    "opel, saab, bus, van = separa_classes(X_treino, y_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(opel, saab, bus, van):\n",
    "    \n",
    "    opsa = np.concatenate( (opel, saab), axis=0)\n",
    "    opbs = np.concatenate( (opel, bus), axis=0)\n",
    "    opvn = np.concatenate( (opel, van), axis=0)\n",
    "    sabu = np.concatenate( (saab, bus), axis=0)\n",
    "    savn = np.concatenate( (saab, van), axis=0)\n",
    "    buva = np.concatenate( (bus, van), axis=0)\n",
    "\n",
    "    label_opel = np.ones( (opel.shape[0]) )\n",
    "    label_saab = np.zeros( (saab.shape[0]) )\n",
    "    label_opsa = np.concatenate( (label_opel, label_saab) )\n",
    "\n",
    "    label_opel = np.ones( (opel.shape[0]) )\n",
    "    label_bus = np.zeros( (bus.shape[0]) )\n",
    "    label_opbs = np.concatenate( (label_opel, label_bus) )\n",
    "\n",
    "    label_opel = np.ones( (opel.shape[0]) )\n",
    "    label_van = np.zeros( (van.shape[0]) )\n",
    "    label_opvn = np.concatenate( (label_opel, label_van) )\n",
    "\n",
    "    label_saab = np.ones( (saab.shape[0]) )\n",
    "    label_bus = np.zeros( (bus.shape[0]) )\n",
    "    label_sabu = np.concatenate( (label_saab, label_bus) )\n",
    "\n",
    "    label_saab = np.ones( (saab.shape[0]) )\n",
    "    label_van = np.zeros( (van.shape[0]) )\n",
    "    label_savn = np.concatenate( (label_saab, label_van) ) \n",
    "\n",
    "    label_bus = np.ones( (bus.shape[0]) )\n",
    "    label_van = np.zeros( (van.shape[0]) )\n",
    "    label_buva = np.concatenate( (label_bus, label_van) )\n",
    "\n",
    "    opel_saab = SGDRegression()\n",
    "    opel_saab.fit(opsa, label_opsa)\n",
    "    \n",
    "    opel_bus = SGDRegression()\n",
    "    opel_bus.fit(opbs, label_opbs)\n",
    "    \n",
    "    opel_van = SGDRegression()\n",
    "    opel_van.fit(opvn, label_opvn)\n",
    "    \n",
    "    saab_bus = SGDRegression()\n",
    "    saab_bus.fit(sabu, label_sabu)\n",
    "    \n",
    "    saab_van = SGDRegression()\n",
    "    saab_van.fit(savn, label_savn)\n",
    "    \n",
    "    bus_van = SGDRegression()\n",
    "    bus_van.fit(buva, label_buva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 0.6931271807599427\n",
      "Iteration: 1 Loss: 0.6886000501538717\n",
      "Iteration: 2 Loss: 0.6979026915321873\n",
      "Iteration: 3 Loss: 0.6781397903298515\n",
      "Iteration: 4 Loss: 0.678151840959023\n",
      "Iteration: 5 Loss: 0.7036345025281558\n",
      "Iteration: 6 Loss: 0.6779675570335761\n",
      "Iteration: 7 Loss: 0.7069228798662874\n",
      "Iteration: 8 Loss: 0.6743396707988715\n",
      "Iteration: 9 Loss: 0.671754688358608\n",
      "Iteration: 10 Loss: 0.6632836860834426\n",
      "Iteration: 11 Loss: 0.7097953530529392\n",
      "Iteration: 12 Loss: 0.657523571553394\n",
      "Iteration: 13 Loss: 0.718836490756092\n",
      "Iteration: 14 Loss: 0.7256856217959924\n",
      "Iteration: 15 Loss: 0.6422713455284768\n",
      "Iteration: 16 Loss: 0.6403166200795024\n",
      "Iteration: 17 Loss: 0.7281796891266278\n",
      "Iteration: 18 Loss: 0.6483249509104272\n",
      "Iteration: 19 Loss: 0.6404434791272443\n",
      "Iteration: 20 Loss: 0.63642973706506\n",
      "Iteration: 21 Loss: 0.7341642095940646\n",
      "Iteration: 22 Loss: 0.7415821536611827\n",
      "Iteration: 23 Loss: 0.7426167927879719\n",
      "Iteration: 24 Loss: 0.6159396676429435\n",
      "Iteration: 25 Loss: 0.6166166753692847\n",
      "Iteration: 26 Loss: 0.640123935790566\n",
      "Iteration: 27 Loss: 0.7490297261840593\n",
      "Iteration: 28 Loss: 0.6431151094550793\n",
      "Iteration: 29 Loss: 0.7502985402698866\n",
      "Iteration: 30 Loss: 0.6027357628222113\n",
      "Iteration: 31 Loss: 0.7641370267237503\n",
      "Iteration: 32 Loss: 0.6261231369914741\n",
      "Iteration: 33 Loss: 0.7596225801425326\n",
      "Iteration: 34 Loss: 0.7607786251487786\n",
      "Iteration: 35 Loss: 0.7775265244034534\n",
      "Iteration: 36 Loss: 0.7835455210200029\n",
      "Iteration: 37 Loss: 0.7905338788100844\n",
      "Iteration: 38 Loss: 0.7828758923148154\n",
      "Iteration: 39 Loss: 0.78093923828086\n",
      "Iteration: 40 Loss: 0.5595501772099837\n",
      "Iteration: 41 Loss: 0.548095277010746\n",
      "Iteration: 42 Loss: 0.8005707529195406\n",
      "Iteration: 43 Loss: 0.5965387561514741\n",
      "Iteration: 44 Loss: 0.5675474889479591\n",
      "Iteration: 45 Loss: 0.7967085410718581\n",
      "Iteration: 46 Loss: 0.5991030137671619\n",
      "Iteration: 47 Loss: 0.8225284664107922\n",
      "Iteration: 48 Loss: 0.8232329644890719\n",
      "Iteration: 49 Loss: 0.8295099204691393\n",
      "Iteration: 50 Loss: 0.5736479588107094\n",
      "Iteration: 51 Loss: 0.8329171058457866\n",
      "Iteration: 52 Loss: 0.5168865749240941\n",
      "Iteration: 53 Loss: 0.5580997255615591\n",
      "Iteration: 54 Loss: 0.5648748642778751\n",
      "Iteration: 55 Loss: 0.8197818999593134\n",
      "Iteration: 56 Loss: 0.8536866528782774\n",
      "Iteration: 57 Loss: 0.8156285463073143\n",
      "Iteration: 58 Loss: 0.8427347037107727\n",
      "Iteration: 59 Loss: 0.8623332369626736\n",
      "Iteration: 60 Loss: 0.5641457642058628\n",
      "Iteration: 61 Loss: 0.4908945729386818\n",
      "Iteration: 62 Loss: 0.8474254275953381\n",
      "Iteration: 63 Loss: 0.5601512394089598\n",
      "Iteration: 64 Loss: 0.8794080301247208\n",
      "Iteration: 65 Loss: 0.5563793094306896\n",
      "Iteration: 66 Loss: 0.8883074050489557\n",
      "Iteration: 67 Loss: 0.8889410566626392\n",
      "Iteration: 68 Loss: 0.503864936100416\n",
      "Iteration: 69 Loss: 0.8796592556334055\n",
      "Iteration: 70 Loss: 0.9041028401293659\n",
      "Iteration: 71 Loss: 0.45885524995145743\n",
      "Iteration: 72 Loss: 0.9119617307638219\n",
      "Iteration: 73 Loss: 0.9156041795465385\n",
      "Iteration: 74 Loss: 0.8908367246886296\n",
      "Iteration: 75 Loss: 0.4306316381054607\n",
      "Iteration: 76 Loss: 0.43608672586506847\n",
      "Iteration: 77 Loss: 0.8958109933433356\n",
      "Iteration: 78 Loss: 0.42662521364145145\n",
      "Iteration: 79 Loss: 0.45089281802112924\n",
      "Iteration: 80 Loss: 0.5231904292468087\n",
      "Iteration: 81 Loss: 0.5353984626144553\n",
      "Iteration: 82 Loss: 0.895036326282145\n",
      "Iteration: 83 Loss: 0.5221810144627536\n",
      "Iteration: 84 Loss: 0.42294094538867244\n",
      "Iteration: 85 Loss: 0.5233147148596897\n",
      "Iteration: 86 Loss: 0.4895230667046056\n",
      "Iteration: 87 Loss: 0.9462683176204812\n",
      "Iteration: 88 Loss: 0.9099137698438763\n",
      "Iteration: 89 Loss: 0.5064105409826333\n",
      "Iteration: 90 Loss: 0.9504667306013564\n",
      "Iteration: 91 Loss: 0.8921742092042005\n",
      "Iteration: 92 Loss: 0.9378074433760322\n",
      "Iteration: 93 Loss: 0.4254490131261451\n",
      "Iteration: 94 Loss: 0.9445827486704382\n",
      "Iteration: 95 Loss: 0.40055923637608826\n",
      "Iteration: 96 Loss: 0.9324311884325006\n",
      "Iteration: 97 Loss: 0.40158780249422127\n",
      "Iteration: 98 Loss: 1.0276429299479697\n",
      "Iteration: 99 Loss: 0.38474462305876467\n",
      "Iteration: 0 Loss: 0.6931271807599427\n",
      "Iteration: 1 Loss: 0.6978892530412715\n",
      "Iteration: 2 Loss: 0.6815697148179188\n",
      "Iteration: 3 Loss: 0.6847743230191405\n",
      "Iteration: 4 Loss: 0.7029648238308369\n",
      "Iteration: 5 Loss: 0.708310634705353\n",
      "Iteration: 6 Loss: 0.7141889401552035\n",
      "Iteration: 7 Loss: 0.7150383775435692\n",
      "Iteration: 8 Loss: 0.724486254028595\n",
      "Iteration: 9 Loss: 0.7311087263477879\n",
      "Iteration: 10 Loss: 0.6548546864631739\n",
      "Iteration: 11 Loss: 0.6518548601337933\n",
      "Iteration: 12 Loss: 0.6556319358396869\n",
      "Iteration: 13 Loss: 0.73599648238315\n",
      "Iteration: 14 Loss: 0.7506568463515749\n",
      "Iteration: 15 Loss: 0.7485543203023608\n",
      "Iteration: 16 Loss: 0.7554487145224879\n",
      "Iteration: 17 Loss: 0.7489819664151034\n",
      "Iteration: 18 Loss: 0.7532618561233857\n",
      "Iteration: 19 Loss: 0.7523285106560056\n",
      "Iteration: 20 Loss: 0.786089606807383\n",
      "Iteration: 21 Loss: 0.6199827043417007\n",
      "Iteration: 22 Loss: 0.5822727854376842\n",
      "Iteration: 23 Loss: 0.5876654792675223\n",
      "Iteration: 24 Loss: 0.7703338908374517\n",
      "Iteration: 25 Loss: 0.786860817839927\n",
      "Iteration: 26 Loss: 0.7810868213239582\n",
      "Iteration: 27 Loss: 0.7988930895554458\n",
      "Iteration: 28 Loss: 0.5804972824597483\n",
      "Iteration: 29 Loss: 0.5553319945082331\n",
      "Iteration: 30 Loss: 0.5938049349818083\n",
      "Iteration: 31 Loss: 0.562657144936487\n",
      "Iteration: 32 Loss: 0.5626570810850354\n",
      "Iteration: 33 Loss: 0.7899350110911496\n",
      "Iteration: 34 Loss: 0.6078918537352993\n",
      "Iteration: 35 Loss: 0.5473119430669795\n",
      "Iteration: 36 Loss: 0.791048983125243\n",
      "Iteration: 37 Loss: 0.5819700285835846\n",
      "Iteration: 38 Loss: 0.5902559194764139\n",
      "Iteration: 39 Loss: 0.6008010395377438\n",
      "Iteration: 40 Loss: 0.5875944707269536\n",
      "Iteration: 41 Loss: 0.545764819720141\n",
      "Iteration: 42 Loss: 0.7860816250156613\n",
      "Iteration: 43 Loss: 0.5670647814374994\n",
      "Iteration: 44 Loss: 0.8065591729095279\n",
      "Iteration: 45 Loss: 0.5797386254032261\n",
      "Iteration: 46 Loss: 0.5733282630588634\n",
      "Iteration: 47 Loss: 0.5475612715192134\n",
      "Iteration: 48 Loss: 0.7939397376219057\n",
      "Iteration: 49 Loss: 0.8017567100437185\n",
      "Iteration: 50 Loss: 0.831707938904253\n",
      "Iteration: 51 Loss: 0.8154697100434254\n",
      "Iteration: 52 Loss: 0.5852203293035413\n",
      "Iteration: 53 Loss: 0.8377917462851248\n",
      "Iteration: 54 Loss: 0.5019716491181406\n",
      "Iteration: 55 Loss: 0.5052206436189424\n",
      "Iteration: 56 Loss: 0.5039637595658122\n",
      "Iteration: 57 Loss: 0.815565590140495\n",
      "Iteration: 58 Loss: 0.8153955115713954\n",
      "Iteration: 59 Loss: 0.8624478588682386\n",
      "Iteration: 60 Loss: 0.8697567993780193\n",
      "Iteration: 61 Loss: 0.5514929573838708\n",
      "Iteration: 62 Loss: 0.8647643825885177\n",
      "Iteration: 63 Loss: 0.8830123036772515\n",
      "Iteration: 64 Loss: 0.8931327307169108\n",
      "Iteration: 65 Loss: 0.8429773865902984\n",
      "Iteration: 66 Loss: 0.5441424249358919\n",
      "Iteration: 67 Loss: 0.874797975403997\n",
      "Iteration: 68 Loss: 0.876430559000707\n",
      "Iteration: 69 Loss: 0.4507044301807426\n",
      "Iteration: 70 Loss: 0.8742021071645026\n",
      "Iteration: 71 Loss: 0.48927826452636664\n",
      "Iteration: 72 Loss: 0.9145445309976803\n",
      "Iteration: 73 Loss: 0.44042700704654497\n",
      "Iteration: 74 Loss: 0.4483554439529809\n",
      "Iteration: 75 Loss: 0.44239950156700536\n",
      "Iteration: 76 Loss: 0.49647415888151214\n",
      "Iteration: 77 Loss: 0.5299912660278749\n",
      "Iteration: 78 Loss: 0.5387167750058095\n",
      "Iteration: 79 Loss: 0.4554803967007788\n",
      "Iteration: 80 Loss: 0.9004681520274048\n",
      "Iteration: 81 Loss: 0.8884507332851106\n",
      "Iteration: 82 Loss: 0.509858820423718\n",
      "Iteration: 83 Loss: 0.4390113938494049\n",
      "Iteration: 84 Loss: 0.4304809084241342\n",
      "Iteration: 85 Loss: 0.9203862439746108\n",
      "Iteration: 86 Loss: 0.4927156126043561\n",
      "Iteration: 87 Loss: 0.48149701593480043\n",
      "Iteration: 88 Loss: 0.917045434503036\n",
      "Iteration: 89 Loss: 0.4485413605346482\n",
      "Iteration: 90 Loss: 1.000744459274712\n",
      "Iteration: 91 Loss: 0.9100139392495391\n",
      "Iteration: 92 Loss: 0.9449191118031361\n",
      "Iteration: 93 Loss: 0.49157698906342684\n",
      "Iteration: 94 Loss: 0.48147687152349405\n",
      "Iteration: 95 Loss: 0.9157074238324083\n",
      "Iteration: 96 Loss: 0.9235612898180195\n",
      "Iteration: 97 Loss: 0.9683028388395247\n",
      "Iteration: 98 Loss: 0.453086741925404\n",
      "Iteration: 99 Loss: 0.9671894046617686\n",
      "Iteration: 0 Loss: 0.6931271807599427\n",
      "Iteration: 1 Loss: 0.6881450827800243\n",
      "Iteration: 2 Loss: 0.6867319666586356\n",
      "Iteration: 3 Loss: 0.6889552685190902\n",
      "Iteration: 4 Loss: 0.6972968408102335\n",
      "Iteration: 5 Loss: 0.6842515258109536\n",
      "Iteration: 6 Loss: 0.7019406191055785\n",
      "Iteration: 7 Loss: 0.6792220698764475\n",
      "Iteration: 8 Loss: 0.6739007199829192\n",
      "Iteration: 9 Loss: 0.6734465273925947\n",
      "Iteration: 10 Loss: 0.7066859948874084\n",
      "Iteration: 11 Loss: 0.7128163765628747\n",
      "Iteration: 12 Loss: 0.7179464800189351\n",
      "Iteration: 13 Loss: 0.7176181078003278\n",
      "Iteration: 14 Loss: 0.6608020273216956\n",
      "Iteration: 15 Loss: 0.6611714359843163\n",
      "Iteration: 16 Loss: 0.659094434105286\n",
      "Iteration: 17 Loss: 0.7299955739937408\n",
      "Iteration: 18 Loss: 0.6425092664246455\n",
      "Iteration: 19 Loss: 0.6273872747787526\n",
      "Iteration: 20 Loss: 0.6490346425076827\n",
      "Iteration: 21 Loss: 0.7343370925128158\n",
      "Iteration: 22 Loss: 0.6503332205930225\n",
      "Iteration: 23 Loss: 0.6215830381388426\n",
      "Iteration: 24 Loss: 0.643710525694052\n",
      "Iteration: 25 Loss: 0.7292923401849608\n",
      "Iteration: 26 Loss: 0.7376889790923384\n",
      "Iteration: 27 Loss: 0.6430989360066676\n",
      "Iteration: 28 Loss: 0.7480602386774946\n",
      "Iteration: 29 Loss: 0.7457732251217156\n",
      "Iteration: 30 Loss: 0.5967786786396451\n",
      "Iteration: 31 Loss: 0.7497702347750869\n",
      "Iteration: 32 Loss: 0.7488519585791736\n",
      "Iteration: 33 Loss: 0.625648356342566\n",
      "Iteration: 34 Loss: 0.6245632129685634\n",
      "Iteration: 35 Loss: 0.7573896378205717\n",
      "Iteration: 36 Loss: 0.619719668911872\n",
      "Iteration: 37 Loss: 0.7588150437680081\n",
      "Iteration: 38 Loss: 0.568387394643715\n",
      "Iteration: 39 Loss: 0.7677272099149876\n",
      "Iteration: 40 Loss: 0.6120230414224975\n",
      "Iteration: 41 Loss: 0.6138820149275762\n",
      "Iteration: 42 Loss: 0.6136151112981051\n",
      "Iteration: 43 Loss: 0.7693000304399992\n",
      "Iteration: 44 Loss: 0.7829598676982092\n",
      "Iteration: 45 Loss: 0.7728403986301879\n",
      "Iteration: 46 Loss: 0.5684464319558583\n",
      "Iteration: 47 Loss: 0.7873491031807053\n",
      "Iteration: 48 Loss: 0.8218727058139735\n",
      "Iteration: 49 Loss: 0.5811291363873972\n",
      "Iteration: 50 Loss: 0.814622075122173\n",
      "Iteration: 51 Loss: 0.8020373661060793\n",
      "Iteration: 52 Loss: 0.5821923818368349\n",
      "Iteration: 53 Loss: 0.5664450251760516\n",
      "Iteration: 54 Loss: 0.8244417526617399\n",
      "Iteration: 55 Loss: 0.5779228941988873\n",
      "Iteration: 56 Loss: 0.5624199636191051\n",
      "Iteration: 57 Loss: 0.796659242112189\n",
      "Iteration: 58 Loss: 0.8149424977804084\n",
      "Iteration: 59 Loss: 0.8325888322743408\n",
      "Iteration: 60 Loss: 0.5572784117820754\n",
      "Iteration: 61 Loss: 0.5642927486776126\n",
      "Iteration: 62 Loss: 0.8299667988131947\n",
      "Iteration: 63 Loss: 0.8350285004126412\n",
      "Iteration: 64 Loss: 0.5393957017717906\n",
      "Iteration: 65 Loss: 0.8573197182199263\n",
      "Iteration: 66 Loss: 0.5461404249267995\n",
      "Iteration: 67 Loss: 0.8383524026035032\n",
      "Iteration: 68 Loss: 0.8730003343747025\n",
      "Iteration: 69 Loss: 0.9144535356387095\n",
      "Iteration: 70 Loss: 0.8783288500522435\n",
      "Iteration: 71 Loss: 0.8943226841303894\n",
      "Iteration: 72 Loss: 0.8945110192744928\n",
      "Iteration: 73 Loss: 0.8696340236626336\n",
      "Iteration: 74 Loss: 0.8763940562761285\n",
      "Iteration: 75 Loss: 0.9062807608929432\n",
      "Iteration: 76 Loss: 0.5124957770794742\n",
      "Iteration: 77 Loss: 0.9212558652063856\n",
      "Iteration: 78 Loss: 0.5123909108831173\n",
      "Iteration: 79 Loss: 0.506627678564264\n",
      "Iteration: 80 Loss: 0.9001422541352602\n",
      "Iteration: 81 Loss: 0.9344631878633048\n",
      "Iteration: 82 Loss: 0.9336076657410123\n",
      "Iteration: 83 Loss: 0.9144984775953322\n",
      "Iteration: 84 Loss: 0.5008737415156805\n",
      "Iteration: 85 Loss: 0.9090667851189156\n",
      "Iteration: 86 Loss: 0.40113956560842073\n",
      "Iteration: 87 Loss: 0.9228571134253835\n",
      "Iteration: 88 Loss: 0.46619195611182057\n",
      "Iteration: 89 Loss: 0.4858116428914099\n",
      "Iteration: 90 Loss: 0.48811107189806663\n",
      "Iteration: 91 Loss: 0.9566431698181043\n",
      "Iteration: 92 Loss: 0.9669367763880572\n",
      "Iteration: 93 Loss: 0.9299517014619934\n",
      "Iteration: 94 Loss: 0.9470920316510358\n",
      "Iteration: 95 Loss: 0.47463408850481764\n",
      "Iteration: 96 Loss: 0.3601012502351748\n",
      "Iteration: 97 Loss: 0.9493133011258506\n",
      "Iteration: 98 Loss: 0.9906936798440882\n",
      "Iteration: 99 Loss: 0.9848120656716397\n",
      "Iteration: 0 Loss: 0.6931271807599427\n",
      "Iteration: 1 Loss: 0.6931270985247486\n",
      "Iteration: 2 Loss: 0.6931270162895683\n",
      "Iteration: 3 Loss: 0.6931269340544013\n",
      "Iteration: 4 Loss: 0.693126851819248\n",
      "Iteration: 5 Loss: 0.693126769584108\n",
      "Iteration: 6 Loss: 0.6931266873489815\n",
      "Iteration: 7 Loss: 0.6931266051138688\n",
      "Iteration: 8 Loss: 0.6931278386415488\n",
      "Iteration: 9 Loss: 0.6823797544999837\n",
      "Iteration: 10 Loss: 0.7023773095410522\n",
      "Iteration: 11 Loss: 0.7059852976958948\n",
      "Iteration: 12 Loss: 0.6750855844953678\n",
      "Iteration: 13 Loss: 0.6736849919253445\n",
      "Iteration: 14 Loss: 0.671373950011836\n",
      "Iteration: 15 Loss: 0.7153062344777913\n",
      "Iteration: 16 Loss: 0.7310248492154391\n",
      "Iteration: 17 Loss: 0.6639945674906308\n",
      "Iteration: 18 Loss: 0.7226283814478067\n",
      "Iteration: 19 Loss: 0.6386686451765355\n",
      "Iteration: 20 Loss: 0.653326325273596\n",
      "Iteration: 21 Loss: 0.7334682378548847\n",
      "Iteration: 22 Loss: 0.6315853752636521\n",
      "Iteration: 23 Loss: 0.7260143402473893\n",
      "Iteration: 24 Loss: 0.6260001435142546\n",
      "Iteration: 25 Loss: 0.7631539959270669\n",
      "Iteration: 26 Loss: 0.6321483627343324\n",
      "Iteration: 27 Loss: 0.6164594853695049\n",
      "Iteration: 28 Loss: 0.6169008382292476\n",
      "Iteration: 29 Loss: 0.6215452048466886\n",
      "Iteration: 30 Loss: 0.7629879279582062\n",
      "Iteration: 31 Loss: 0.6057740388507084\n",
      "Iteration: 32 Loss: 0.6057083187690733\n",
      "Iteration: 33 Loss: 0.7524509885662672\n",
      "Iteration: 34 Loss: 0.6007325720878903\n",
      "Iteration: 35 Loss: 0.6054551978511137\n",
      "Iteration: 36 Loss: 0.7556915845833414\n",
      "Iteration: 37 Loss: 0.7535029750619527\n",
      "Iteration: 38 Loss: 0.791857649678534\n",
      "Iteration: 39 Loss: 0.5785816349925545\n",
      "Iteration: 40 Loss: 0.5868766645923145\n",
      "Iteration: 41 Loss: 0.7744669558347306\n",
      "Iteration: 42 Loss: 0.5795838666188222\n",
      "Iteration: 43 Loss: 0.585463040298316\n",
      "Iteration: 44 Loss: 0.8184993479845222\n",
      "Iteration: 45 Loss: 0.7913264170749857\n",
      "Iteration: 46 Loss: 0.8545608368288596\n",
      "Iteration: 47 Loss: 0.868088223355016\n",
      "Iteration: 48 Loss: 0.5406208247526398\n",
      "Iteration: 49 Loss: 0.8192255824357557\n",
      "Iteration: 50 Loss: 0.805052869176838\n",
      "Iteration: 51 Loss: 0.584519135129871\n",
      "Iteration: 52 Loss: 0.5754364619634261\n",
      "Iteration: 53 Loss: 0.5237152237216309\n",
      "Iteration: 54 Loss: 0.5251858584167107\n",
      "Iteration: 55 Loss: 0.89264335716323\n",
      "Iteration: 56 Loss: 0.9185160238829727\n",
      "Iteration: 57 Loss: 0.5154682441558346\n",
      "Iteration: 58 Loss: 0.9222113247904992\n",
      "Iteration: 59 Loss: 0.5009259364638003\n",
      "Iteration: 60 Loss: 0.9504229895739468\n",
      "Iteration: 61 Loss: 0.5781502093618179\n",
      "Iteration: 62 Loss: 0.838175135606932\n",
      "Iteration: 63 Loss: 0.48601291087193466\n",
      "Iteration: 64 Loss: 0.5466046097498805\n",
      "Iteration: 65 Loss: 0.9280749867447351\n",
      "Iteration: 66 Loss: 0.8584149248399934\n",
      "Iteration: 67 Loss: 0.5477997398715999\n",
      "Iteration: 68 Loss: 1.0010734580306928\n",
      "Iteration: 69 Loss: 0.4752834650870329\n",
      "Iteration: 70 Loss: 0.8811704709125912\n",
      "Iteration: 71 Loss: 0.9779108757340396\n",
      "Iteration: 72 Loss: 1.030688348494076\n",
      "Iteration: 73 Loss: 0.44459132100813437\n",
      "Iteration: 74 Loss: 0.8679939628096595\n",
      "Iteration: 75 Loss: 0.49137794901439813\n",
      "Iteration: 76 Loss: 0.4962512816984586\n",
      "Iteration: 77 Loss: 0.9243750063261243\n",
      "Iteration: 78 Loss: 0.8788674231420619\n",
      "Iteration: 79 Loss: 1.0699940261948242\n",
      "Iteration: 80 Loss: 0.42578679244838913\n",
      "Iteration: 81 Loss: 0.9042266751660414\n",
      "Iteration: 82 Loss: 1.035343591482459\n",
      "Iteration: 83 Loss: 1.0039736049387475\n",
      "Iteration: 84 Loss: 0.4549216621863206\n",
      "Iteration: 85 Loss: 0.4249795656898377\n",
      "Iteration: 86 Loss: 0.47681400339607183\n",
      "Iteration: 87 Loss: 0.39700719004449286\n",
      "Iteration: 88 Loss: 0.5197981694972527\n",
      "Iteration: 89 Loss: 1.1108432146334235\n",
      "Iteration: 90 Loss: 0.3912397867061883\n",
      "Iteration: 91 Loss: 0.5243373931652533\n",
      "Iteration: 92 Loss: 1.1377337526518518\n",
      "Iteration: 93 Loss: 1.1617960205749178\n",
      "Iteration: 94 Loss: 0.5079155587743707\n",
      "Iteration: 95 Loss: 0.40162766651227055\n",
      "Iteration: 96 Loss: 0.4718977702180285\n",
      "Iteration: 97 Loss: 0.9943445187051068\n",
      "Iteration: 98 Loss: 0.4530548279086519\n",
      "Iteration: 99 Loss: 1.170828244111944\n",
      "Iteration: 0 Loss: 0.6931271807599427\n",
      "Iteration: 1 Loss: 0.6931272635397476\n",
      "Iteration: 2 Loss: 0.6865452446382168\n",
      "Iteration: 3 Loss: 0.7008471934946263\n",
      "Iteration: 4 Loss: 0.7085679616319245\n",
      "Iteration: 5 Loss: 0.6593640044779722\n",
      "Iteration: 6 Loss: 0.7127974363946823\n",
      "Iteration: 7 Loss: 0.7320050929545416\n",
      "Iteration: 8 Loss: 0.7399111483509035\n",
      "Iteration: 9 Loss: 0.6368455817133933\n",
      "Iteration: 10 Loss: 0.657158519165355\n",
      "Iteration: 11 Loss: 0.6568260131118268\n",
      "Iteration: 12 Loss: 0.7481932308576883\n",
      "Iteration: 13 Loss: 0.7646792598230221\n",
      "Iteration: 14 Loss: 0.6417144641535337\n",
      "Iteration: 15 Loss: 0.7585236295345033\n",
      "Iteration: 16 Loss: 0.7905408344929911\n",
      "Iteration: 17 Loss: 0.8073461646605686\n",
      "Iteration: 18 Loss: 0.7868223728887145\n",
      "Iteration: 19 Loss: 0.6184284292385891\n",
      "Iteration: 20 Loss: 0.8248431682864463\n",
      "Iteration: 21 Loss: 0.841330178396635\n",
      "Iteration: 22 Loss: 0.8511351962540461\n",
      "Iteration: 23 Loss: 0.5935021225328464\n",
      "Iteration: 24 Loss: 0.586020259847324\n",
      "Iteration: 25 Loss: 0.6017097362991928\n",
      "Iteration: 26 Loss: 0.8647607251092622\n",
      "Iteration: 27 Loss: 0.8759651245084736\n",
      "Iteration: 28 Loss: 0.8761130059896162\n",
      "Iteration: 29 Loss: 0.5821941850192797\n",
      "Iteration: 30 Loss: 0.5768919924413582\n",
      "Iteration: 31 Loss: 0.5796469929928058\n",
      "Iteration: 32 Loss: 0.5629684124183569\n",
      "Iteration: 33 Loss: 0.5811659498828731\n",
      "Iteration: 34 Loss: 0.7863642502153935\n",
      "Iteration: 35 Loss: 0.577819323176548\n",
      "Iteration: 36 Loss: 0.5738373628361415\n",
      "Iteration: 37 Loss: 0.5703053350684525\n",
      "Iteration: 38 Loss: 0.5780506681621941\n",
      "Iteration: 39 Loss: 0.5764590784277335\n",
      "Iteration: 40 Loss: 0.839130612800097\n",
      "Iteration: 41 Loss: 0.5117483384171743\n",
      "Iteration: 42 Loss: 0.900866825717188\n",
      "Iteration: 43 Loss: 0.91819239213605\n",
      "Iteration: 44 Loss: 0.5608366924271257\n",
      "Iteration: 45 Loss: 0.44247625556002557\n",
      "Iteration: 46 Loss: 0.8870577554663219\n",
      "Iteration: 47 Loss: 0.9409073760277348\n",
      "Iteration: 48 Loss: 0.9798236594426296\n",
      "Iteration: 49 Loss: 0.9224962403003233\n",
      "Iteration: 50 Loss: 0.5385607959659539\n",
      "Iteration: 51 Loss: 0.4337758767242\n",
      "Iteration: 52 Loss: 1.0031723491891962\n",
      "Iteration: 53 Loss: 0.5272993416685342\n",
      "Iteration: 54 Loss: 0.5301143317865211\n",
      "Iteration: 55 Loss: 0.9282000026057724\n",
      "Iteration: 56 Loss: 0.5222214577625589\n",
      "Iteration: 57 Loss: 0.5143466008723739\n",
      "Iteration: 58 Loss: 1.0369657161458363\n",
      "Iteration: 59 Loss: 0.37962083464448215\n",
      "Iteration: 60 Loss: 0.5096153235811046\n",
      "Iteration: 61 Loss: 0.5079495554729424\n",
      "Iteration: 62 Loss: 1.0370806123776293\n",
      "Iteration: 63 Loss: 0.4043866029578167\n",
      "Iteration: 64 Loss: 0.9872330134965421\n",
      "Iteration: 65 Loss: 0.5002309028486801\n",
      "Iteration: 66 Loss: 0.4475211784432825\n",
      "Iteration: 67 Loss: 0.9904500102123596\n",
      "Iteration: 68 Loss: 0.9827581522383185\n",
      "Iteration: 69 Loss: 1.0839794242796483\n",
      "Iteration: 70 Loss: 0.4731435196378814\n",
      "Iteration: 71 Loss: 0.4996035618641871\n",
      "Iteration: 72 Loss: 0.502072879740747\n",
      "Iteration: 73 Loss: 0.4679328934616287\n",
      "Iteration: 74 Loss: 0.4946736730441439\n",
      "Iteration: 75 Loss: 0.49972654072828626\n",
      "Iteration: 76 Loss: 0.38568947452396246\n",
      "Iteration: 77 Loss: 1.117466578448323\n",
      "Iteration: 78 Loss: 1.0612756250487276\n",
      "Iteration: 79 Loss: 0.49099909494722777\n",
      "Iteration: 80 Loss: 0.49087785930054506\n",
      "Iteration: 81 Loss: 1.075250146563628\n",
      "Iteration: 82 Loss: 0.4870051087177244\n",
      "Iteration: 83 Loss: 0.4829131291615261\n",
      "Iteration: 84 Loss: 1.007568703268676\n",
      "Iteration: 85 Loss: 1.0454742392099636\n",
      "Iteration: 86 Loss: 0.4795865755139148\n",
      "Iteration: 87 Loss: 1.1073418934044033\n",
      "Iteration: 88 Loss: 0.9463104733121924\n",
      "Iteration: 89 Loss: 1.1938741466375167\n",
      "Iteration: 90 Loss: 0.9777834109448172\n",
      "Iteration: 91 Loss: 1.21771572068528\n",
      "Iteration: 92 Loss: 0.4520498772403132\n",
      "Iteration: 93 Loss: 1.0026920154802248\n",
      "Iteration: 94 Loss: 1.1867531827486055\n",
      "Iteration: 95 Loss: 0.966778716931617\n",
      "Iteration: 96 Loss: 1.086098117727866\n",
      "Iteration: 97 Loss: 0.9795141731479324\n",
      "Iteration: 98 Loss: 0.4328895222484598\n",
      "Iteration: 99 Loss: 1.301687219326772\n",
      "Iteration: 0 Loss: 0.6931271807599427\n",
      "Iteration: 1 Loss: 0.6882677691821263\n",
      "Iteration: 2 Loss: 0.6885773210297819\n",
      "Iteration: 3 Loss: 0.6976690242663707\n",
      "Iteration: 4 Loss: 0.6839037101388499\n",
      "Iteration: 5 Loss: 0.7015756051661629\n",
      "Iteration: 6 Loss: 0.6800515127021773\n",
      "Iteration: 7 Loss: 0.6777938312149434\n",
      "Iteration: 8 Loss: 0.7072040544281921\n",
      "Iteration: 9 Loss: 0.7138569735505037\n",
      "Iteration: 10 Loss: 0.717704066591985\n",
      "Iteration: 11 Loss: 0.7322677156304968\n",
      "Iteration: 12 Loss: 0.658192127903939\n",
      "Iteration: 13 Loss: 0.654481510846431\n",
      "Iteration: 14 Loss: 0.7193945008534898\n",
      "Iteration: 15 Loss: 0.6495271970852822\n",
      "Iteration: 16 Loss: 0.752106859421466\n",
      "Iteration: 17 Loss: 0.7442853190042535\n",
      "Iteration: 18 Loss: 0.7647134186078712\n",
      "Iteration: 19 Loss: 0.7565098206994614\n",
      "Iteration: 20 Loss: 0.6237924628031966\n",
      "Iteration: 21 Loss: 0.6339969119654478\n",
      "Iteration: 22 Loss: 0.7740391543250711\n",
      "Iteration: 23 Loss: 0.7611922153485816\n",
      "Iteration: 24 Loss: 0.7676170750412594\n",
      "Iteration: 25 Loss: 0.8101529763174704\n",
      "Iteration: 26 Loss: 0.8150112506369845\n",
      "Iteration: 27 Loss: 0.8439179078928793\n",
      "Iteration: 28 Loss: 0.7980255760550826\n",
      "Iteration: 29 Loss: 0.5845157820759167\n",
      "Iteration: 30 Loss: 0.8157067849332066\n",
      "Iteration: 31 Loss: 0.7949784563071097\n",
      "Iteration: 32 Loss: 0.8710760783363346\n",
      "Iteration: 33 Loss: 0.8485478490708771\n",
      "Iteration: 34 Loss: 0.5609186138245111\n",
      "Iteration: 35 Loss: 0.5391996513693071\n",
      "Iteration: 36 Loss: 0.575810430871508\n",
      "Iteration: 37 Loss: 0.5787538459942081\n",
      "Iteration: 38 Loss: 0.8242740884179293\n",
      "Iteration: 39 Loss: 0.5169113696653308\n",
      "Iteration: 40 Loss: 0.5754537994715971\n",
      "Iteration: 41 Loss: 0.5521942888117738\n",
      "Iteration: 42 Loss: 0.794477289891868\n",
      "Iteration: 43 Loss: 0.5497140904162543\n",
      "Iteration: 44 Loss: 0.8931309968785383\n",
      "Iteration: 45 Loss: 0.9218788466202478\n",
      "Iteration: 46 Loss: 0.501500545012945\n",
      "Iteration: 47 Loss: 0.5610991111589081\n",
      "Iteration: 48 Loss: 0.9516330362816064\n",
      "Iteration: 49 Loss: 0.896131986776829\n",
      "Iteration: 50 Loss: 0.5461681809800272\n",
      "Iteration: 51 Loss: 0.8989758084854996\n",
      "Iteration: 52 Loss: 0.5021355995745487\n",
      "Iteration: 53 Loss: 0.4474024567350372\n",
      "Iteration: 54 Loss: 0.9640258916867077\n",
      "Iteration: 55 Loss: 1.0003085320376142\n",
      "Iteration: 56 Loss: 0.465978886326724\n",
      "Iteration: 57 Loss: 0.5360668926084217\n",
      "Iteration: 58 Loss: 0.9988234807132718\n",
      "Iteration: 59 Loss: 0.9484387941468477\n",
      "Iteration: 60 Loss: 1.033675951280236\n",
      "Iteration: 61 Loss: 0.9545776588251083\n",
      "Iteration: 62 Loss: 1.0589387106776038\n",
      "Iteration: 63 Loss: 0.9269316786966308\n",
      "Iteration: 64 Loss: 0.9410280545048757\n",
      "Iteration: 65 Loss: 0.504840024886314\n",
      "Iteration: 66 Loss: 0.4825020782293408\n",
      "Iteration: 67 Loss: 0.8659701969476417\n",
      "Iteration: 68 Loss: 1.0821400574603737\n",
      "Iteration: 69 Loss: 1.0460738086721715\n",
      "Iteration: 70 Loss: 0.48574473251194356\n",
      "Iteration: 71 Loss: 1.0977690947188474\n",
      "Iteration: 72 Loss: 0.48451390814358214\n",
      "Iteration: 73 Loss: 0.48644975375124594\n",
      "Iteration: 74 Loss: 1.1403767850327842\n",
      "Iteration: 75 Loss: 1.151710873625076\n",
      "Iteration: 76 Loss: 0.46943815530174665\n",
      "Iteration: 77 Loss: 0.4700753069232029\n",
      "Iteration: 78 Loss: 1.1487529534020064\n",
      "Iteration: 79 Loss: 0.44952166355508894\n",
      "Iteration: 80 Loss: 0.4496778949251674\n",
      "Iteration: 81 Loss: 0.3025810156605315\n",
      "Iteration: 82 Loss: 0.9691713436928845\n",
      "Iteration: 83 Loss: 0.46728653891710015\n",
      "Iteration: 84 Loss: 0.3235105477082213\n",
      "Iteration: 85 Loss: 1.0666811453661593\n",
      "Iteration: 86 Loss: 0.4623692206296047\n",
      "Iteration: 87 Loss: 0.45523129720918787\n",
      "Iteration: 88 Loss: 0.46019501400321117\n",
      "Iteration: 89 Loss: 0.44376892623544345\n",
      "Iteration: 90 Loss: 1.1085987894969391\n",
      "Iteration: 91 Loss: 0.9536162482293006\n",
      "Iteration: 92 Loss: 0.9212559085546189\n",
      "Iteration: 93 Loss: 0.45601494031460593\n",
      "Iteration: 94 Loss: 1.229618621493448\n",
      "Iteration: 95 Loss: 0.31412711128436654\n",
      "Iteration: 96 Loss: 1.239077674226273\n",
      "Iteration: 97 Loss: 0.3927252804361573\n",
      "Iteration: 98 Loss: 0.44601278636033914\n",
      "Iteration: 99 Loss: 0.445798242511189\n"
     ]
    }
   ],
   "source": [
    "models(opel, saab, bus, van)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def votacao(label):\n",
    "    \n",
    "    if label == \"van\":\n",
    "        if pred[2] +  pred[4] + pred[5] < 2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif label == \"opel\":\n",
    "        if pred[0] +  pred[1] + pred[2] > 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif label == \"bus\":\n",
    "        if pred[1] +  pred[3] + pred[5] < 2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if pred[2] +  pred[4] + pred[5] < 2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(846, 19)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vehicle = pd.read_csv('dataset_vehicle.csv')\n",
    "data_vehicle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breaking the data into training and test set\n",
    "import random\n",
    "def train_test_split(data, split, trainingSet = [], testSet = []):\n",
    "    for x in range(len(data)):\n",
    "        if random.random() < split:\n",
    "            trainingSet.append(data[x])\n",
    "        else:\n",
    "            testSet.append(data[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_vehicle.values.tolist()\n",
    "trainingSet = []\n",
    "testSet = []\n",
    "split = 0.7\n",
    "train_test_split(df, split, trainingSet, testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Euclidean distances\n",
    "import math\n",
    "def Euclideandist(x,xi, length):\n",
    "    d = 0.0\n",
    "    for i in range(length):\n",
    "        d += pow(float(x[i])- float(xi[i]),2)\n",
    "    return math.sqrt(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bus</th>\n",
       "      <th>van</th>\n",
       "      <th>opel</th>\n",
       "      <th>saab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bus</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>van</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opel</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saab</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bus  van  opel  saab\n",
       "bus     0    0     0     0\n",
       "van     0    0     0     0\n",
       "opel    0    0     0     0\n",
       "saab    0    0     0     0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = pd.DataFrame(\n",
    "    {\n",
    "        \"bus\": [0, 0, 0, 0],\n",
    "        \"van\": [0, 0, 0, 0],\n",
    "        \"opel\": [0, 0, 0, 0],\n",
    "        \"saab\": [0, 0, 0, 0],\n",
    "    }\n",
    ")\n",
    "\n",
    "confusion_matrix = confusion_matrix.rename(index = {0: \"bus\", 1: \"van\", 2: \"opel\", 3: \"saab\"})\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> predicted='van', actual='van'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='van', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='bus'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='opel'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='saab'\n",
      "> predicted='van', actual='saab'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='saab'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='bus', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='van'\n",
      "> predicted='bus', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='van'\n",
      "> predicted='opel', actual='bus'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='van', actual='opel'\n",
      "> predicted='van', actual='bus'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='bus', actual='opel'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='opel'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='saab'\n",
      "> predicted='van', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='van', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='opel'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='opel'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='bus', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='bus', actual='saab'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='bus', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='van', actual='opel'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='saab', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='bus', actual='saab'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='saab', actual='opel'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='van', actual='opel'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='opel', actual='bus'\n",
      "> predicted='bus', actual='bus'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='bus', actual='opel'\n",
      "> predicted='bus', actual='opel'\n",
      "> predicted='opel', actual='opel'\n",
      "> predicted='van', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='opel', actual='saab'\n",
      "> predicted='van', actual='van'\n",
      "> predicted='saab', actual='saab'\n",
      "> predicted='saab', actual='saab'\n"
     ]
    }
   ],
   "source": [
    "#Getting the K neighbours having the closest Euclidean distance to the test instance\n",
    "import operator\n",
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "    distances = []\n",
    "    length = len(testInstance)-1\n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = Euclideandist(testInstance, trainingSet[x], length)\n",
    "        distances.append((trainingSet[x], dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors\n",
    "\n",
    "#After sorting the neighbours based on their respective classes, max voting to give the final class of the test instance\n",
    "def getResponse(neighbors):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x][-1]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)#Sorting it based on votes\n",
    "    return sortedVotes[0][0] #Please note we need the class for the top voted class, hence [0][0]#\n",
    "\n",
    "#Getting the accuracy\n",
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0\n",
    "\n",
    "# generate predictions\n",
    "predictions=[]\n",
    "k = 3\n",
    "for x in range(len(testSet)):\n",
    "    neighbors = getNeighbors(trainingSet, testSet[x], k)\n",
    "    result = getResponse(neighbors)\n",
    "    if testSet[x][-1] == 'bus':\n",
    "        confusion_matrix.iloc[0][result]+=1\n",
    "    elif testSet[x][-1] == 'van':\n",
    "        confusion_matrix.iloc[1][result]+=1\n",
    "    elif testSet[x][-1] == 'opel':\n",
    "        confusion_matrix.iloc[2][result]+=1\n",
    "    else:\n",
    "        confusion_matrix.iloc[3][result]+=1\n",
    "    predictions.append(result)\n",
    "    print('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bus</th>\n",
       "      <th>van</th>\n",
       "      <th>opel</th>\n",
       "      <th>saab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bus</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>van</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opel</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saab</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bus  van  opel  saab\n",
       "bus    52    2     3     1\n",
       "van     0   47     1     1\n",
       "opel    8    4    25    24\n",
       "saab    9    3    25    30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.53191489361701%\n"
     ]
    }
   ],
   "source": [
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "colnames_numeric = data_vehicle.columns[0:18]\n",
    "\n",
    "trainingSet2 = pd.DataFrame(np.array(trainingSet).reshape(len(trainingSet),19), columns = data_vehicle.columns)\n",
    "testSet2 = pd.DataFrame(np.array(testSet).reshape(len(testSet),19), columns = data_vehicle.columns)\n",
    "\n",
    "trainingSet2[colnames_numeric] = trainingSet2[colnames_numeric].apply(pd.to_numeric, errors = 'coerce', axis = 0)\n",
    "testSet2[colnames_numeric] = testSet2[colnames_numeric].apply(pd.to_numeric, errors = 'coerce', axis = 0)\n",
    "\n",
    "knn_sklearn = KNeighborsClassifier(n_neighbors = 1, metric='euclidean')\n",
    "x_train,y_train = trainingSet2.loc[:,trainingSet2.columns != 'Class'], trainingSet2.loc[:,'Class']\n",
    "x_test,y_test = testSet2.loc[:,testSet2.columns != 'Class'], testSet2.loc[:,'Class']\n",
    "knn_sklearn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['van' 'van' 'bus' 'van' 'van' 'saab' 'saab' 'bus' 'saab' 'van' 'bus'\n",
      " 'opel' 'bus' 'van' 'saab' 'bus' 'van' 'bus' 'saab' 'bus' 'van' 'opel'\n",
      " 'van' 'saab' 'bus' 'bus' 'van' 'saab' 'opel' 'van' 'bus' 'saab' 'van'\n",
      " 'van' 'bus' 'opel' 'opel' 'saab' 'bus' 'bus' 'van' 'opel' 'bus' 'saab'\n",
      " 'saab' 'bus' 'bus' 'bus' 'opel' 'saab' 'bus' 'opel' 'opel' 'bus' 'bus'\n",
      " 'van' 'bus' 'bus' 'bus' 'van' 'saab' 'van' 'saab' 'bus' 'saab' 'saab'\n",
      " 'opel' 'van' 'van' 'opel' 'opel' 'van' 'bus' 'bus' 'opel' 'bus' 'bus'\n",
      " 'saab' 'opel' 'saab' 'saab' 'saab' 'bus' 'bus' 'bus' 'bus' 'opel' 'van'\n",
      " 'bus' 'opel' 'bus' 'saab' 'bus' 'saab' 'van' 'opel' 'saab' 'saab' 'saab'\n",
      " 'bus' 'bus' 'opel' 'opel' 'saab' 'opel' 'van' 'bus' 'van' 'bus' 'saab'\n",
      " 'opel' 'bus' 'saab' 'saab' 'bus' 'opel' 'saab' 'saab' 'bus' 'van' 'bus'\n",
      " 'van' 'saab' 'bus' 'opel' 'saab' 'van' 'opel' 'bus' 'bus' 'bus' 'bus'\n",
      " 'opel' 'saab' 'saab' 'saab' 'van' 'opel' 'saab' 'opel' 'opel' 'opel'\n",
      " 'opel' 'opel' 'opel' 'opel' 'saab' 'bus' 'van' 'saab' 'opel' 'van' 'saab'\n",
      " 'bus' 'bus' 'van' 'van' 'opel' 'opel' 'bus' 'van' 'saab' 'van' 'van'\n",
      " 'opel' 'opel' 'saab' 'opel' 'opel' 'opel' 'van' 'bus' 'van' 'saab' 'bus'\n",
      " 'opel' 'bus' 'saab' 'saab' 'bus' 'van' 'saab' 'saab' 'bus' 'bus' 'van'\n",
      " 'bus' 'saab' 'opel' 'saab' 'saab' 'van' 'bus' 'saab' 'saab' 'bus' 'bus'\n",
      " 'opel' 'van' 'bus' 'van' 'saab' 'van' 'opel' 'van' 'opel' 'van' 'saab'\n",
      " 'opel' 'opel' 'saab' 'bus' 'bus' 'opel' 'van' 'van' 'saab' 'saab' 'opel'\n",
      " 'bus' 'bus' 'saab' 'saab' 'saab' 'van' 'opel' 'bus' 'bus' 'opel' 'opel'\n",
      " 'opel' 'opel' 'van' 'opel' 'saab']\n",
      "With KNN (K=3) accuracy is:  0.6042553191489362\n"
     ]
    }
   ],
   "source": [
    "prediction = knn_sklearn.predict(x_test)\n",
    "print('Prediction: {}'.format(prediction))\n",
    "print('With KNN (K=3) accuracy is: ',knn_sklearn.score(x_test,y_test)) # accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
